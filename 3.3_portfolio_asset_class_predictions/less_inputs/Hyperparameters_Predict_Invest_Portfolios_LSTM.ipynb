{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e05e13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.005167 using {'batch_size': 2, 'epochs': 100}\n",
      "11/11 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "# Load data from a CSV file\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Apply label encoding to the 'state' column\n",
    "label_encoder = LabelEncoder()\n",
    "df['state'] = label_encoder.fit_transform(df['state'])\n",
    "\n",
    "# 'data' contains columns 'sex', 'age', 'state', 'risk_profile'\n",
    "data = df[['sex', 'age', 'state', 'risk_profile']].values\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# 'targets' contains columns 'a1' through 'a20'\n",
    "targets = df[['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'a10', \n",
    "              'a11', 'a12', 'a13', 'a14', 'a15', 'a16', 'a17', 'a18', 'a19', 'a20']].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split_ratio = 0.7  # 70% for training, 30% for testing\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "x_train, x_test = data[:split_index], data[split_index:]\n",
    "y_train, y_test = targets[:split_index], targets[split_index:]\n",
    "\n",
    "# Reshape the input data to match the expected input shape of the model\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "\n",
    "# Define the model creation function for KerasRegressor\n",
    "def create_model(epochs=25, batch_size=8):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "    model.add(Dense(units=20))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# Create KerasRegressor\n",
    "model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "param_grid = {\n",
    "    'epochs': [25, 50, 75, 100], \n",
    "    'batch_size': [1, 2, 4, 8, 16]\n",
    "}\n",
    "\n",
    "# Create and fit the GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# Use the best parameters to create the final model\n",
    "best_epochs = grid_result.best_params_['epochs']\n",
    "best_batch_size = grid_result.best_params_['batch_size']\n",
    "final_model = create_model(epochs=best_epochs, batch_size=best_batch_size)\n",
    "\n",
    "# Train the final model\n",
    "final_model.fit(x_train, y_train, epochs=best_epochs, batch_size=best_batch_size, validation_data=(x_test, y_test), verbose=0)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = final_model.predict(x_test)\n",
    "\n",
    "# Reshape y_test and y_pred to 2D arrays\n",
    "y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "y_pred = y_pred.reshape(y_pred.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4194e400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred average = 1.0131589519038575\n"
     ]
    }
   ],
   "source": [
    "sums = []\n",
    "for row in y_pred:\n",
    "    sums.append(sum(row))\n",
    "#print('y_pred =', sums)\n",
    "print(\"y_pred average =\", sum(sums)/len(sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e691de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove values < 0.01 and Scale predictions to get 1 (100%) as a sum of each row\n",
    "num_rows = len(y_pred)\n",
    "num_cols = len(y_pred[0])\n",
    "\n",
    "for row in range(num_rows):\n",
    "    for col in range(num_cols):\n",
    "        if y_pred[row][col] < 0.01:\n",
    "            y_pred[row][col] = 0\n",
    "\n",
    "for row in range(num_rows):\n",
    "    row_sum = sum(y_pred[row])\n",
    "    for col in range(num_cols):\n",
    "        y_pred[row][col] = y_pred[row][col] / row_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c3e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred average = 0.9999999984813092\n"
     ]
    }
   ],
   "source": [
    "sums = []\n",
    "for row in y_pred:\n",
    "    sums.append(sum(row))\n",
    "#print('y_pred =', sums)\n",
    "print(\"y_pred average =\", sum(sums)/len(sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d526a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.    41.    33.    ...  0.     0.     0.   ]\n",
      " [ 1.    69.    14.    ...  0.     0.013  0.   ]\n",
      " [ 1.    46.    28.    ...  0.     0.011  0.   ]\n",
      " ...\n",
      " [ 1.    64.    24.    ...  0.     0.011  0.   ]\n",
      " [ 1.    57.    11.    ...  0.     0.012  0.   ]\n",
      " [ 0.    51.    14.    ...  0.     0.012  0.   ]]\n"
     ]
    }
   ],
   "source": [
    "data_for_csv = df[['sex', 'age', 'state', 'risk_profile']].values\n",
    "data_train, data_test = data_for_csv[:split_index], data_for_csv[split_index:]\n",
    "array1 = data_test\n",
    "array2 = np.round(np.array(y_pred), 3)\n",
    "\n",
    "result_array = np.concatenate((array1, array2), axis=1)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(result_array)\n",
    "\n",
    "np.savetxt('HP_LSTM_output.csv', result_array, delimiter=\",\", fmt='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f310a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb64c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6926a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
